{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "\n",
    "by   \n",
    "[__Michael Granitzer__ (michael.granitzer@uni-passau.de)]( http://www.mendeley.com/profiles/michael-granitzer/)  \n",
    "[Konstantin Ziegler (konstantin.ziegler@uni-passau.de)](http://zieglerk.net)  \n",
    "Jörg Schlötterer (joerg.schloetterer@uni-passau.de)\n",
    "\n",
    "with examples taken from the [scikit-learn documentation](http://scikit-learn.org/stable/)\n",
    "\n",
    "__License__\n",
    "\n",
    "This work is licensed under a [Creative Commons Attribution 3.0 Unported License](http://creativecommons.org/licenses/by/3.0/) (CC BY 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "### Data Set\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. The data is organized into 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware / comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale / soc.religion.christian). Here is a list of the 20 newsgroups, partitioned (more or less) according to subject matter:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<table border=1>\n",
    "<tr>\n",
    "<td>comp.graphics<br>comp.os.ms-windows.misc<br>comp.sys.ibm.pc.hardware<br>comp.sys.mac.hardware<br>comp.windows.x</td>\n",
    "<td>rec.autos<br>rec.motorcycles<br>rec.sport.baseball<br>rec.sport.hockey</td>\n",
    "<td>sci.crypt<br>sci.electronics<br>sci.med<br>sci.space</td>\n",
    "</tr><tr>\n",
    "<td>misc.forsale</td>\n",
    "<td>talk.politics.misc<br>talk.politics.guns<br>talk.politics.mideast</td>\n",
    "<td>talk.religion.misc<br>alt.atheism<br>soc.religion.christian</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The \"bydate\"-option is sorted by date into training(60%) and test(40%) sets, does not include cross-posts (duplicates) and does not include newsgroup-identifying headers (Xref, Newsgroups, Path, Followup-To, Date). \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Implement your own naive bayes classifier and apply it to the 20newsgroups dataset.  \n",
    "</div>\n",
    "* Take a look at the [notebook about 20newsgroups](../1-Datasets_Visualization_and_preprocessing/5-20newsgroups.ipynb) to obtain the data\n",
    "* Read the files and tokenize the text to obtain a \"bag of words\"\n",
    "* Implement the naive bayes classifier (pseudocode is given below)\n",
    "* Evaluate your classfier on the training/test set. Which accuracy can you achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Pseudocode\n",
    "#### TrainMultiNomialNB($\\mathbb C$,$\\mathbb D$)  \n",
    "$V \\leftarrow extractVocabulary(\\mathbb D)$  \n",
    "$N \\leftarrow countDocs(\\mathbb D)$    \n",
    "for $c \\in \\mathbb C$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$N_c \\leftarrow countDocsInClass(\\mathbb D, c)$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$prior[c] \\leftarrow \\frac{N_c}{N}$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$text_c \\leftarrow concatenateTextOfAllDocsInClass(\\mathbb D, c)$   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in V$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$T_{ct} \\leftarrow countTokensOfTerm(text_c,t)$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in V$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$condprob[t][c] \\leftarrow \\frac{T_{ct} + 1}{\\sum_{t'}(T_{ct'} + 1)}$  \n",
    "return $V,prior,condprob$\n",
    "\n",
    "#### ApplyMultinomialNB($\\mathbb C,V,prior,condprob,d$)\n",
    "$W \\leftarrow extractTokensFromDoc(V,d)$   \n",
    "for $c \\in \\mathbb C$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$score[c] \\leftarrow log(prior[c])$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in W$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$score[c] += log(condprob[t][c])$  \n",
    "return $argmax_{c \\in \\mathbb C} score[c]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Some snippets that might be useful for the implementation: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'test', 'string']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "import re\n",
    "def tokenize(doc):\n",
    "    return re.findall(r'\\b\\w\\w+\\b',doc) # return all words with #characters > 1\n",
    "\n",
    "tokenize(\"This is a test string.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "# list files (or directories)\n",
    "import os\n",
    "for directory in os.listdir('./../1-Datasets_Visualization_and_preprocessing/newsgroups/20news-bydate-train/'):\n",
    "    print(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \"# Text Classification\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"by   \\n\",\n",
      "    \"[__Michael Granitzer__ (michael.granitzer@uni-passau.de)]( http://www.mendeley.com/profiles/michael-granitzer/)  \\n\",\n",
      "    \"[Konstantin Ziegler (konstantin.ziegler@uni-passau.de)](http://zieglerk.net)  \\n\",\n",
      "    \"Jörg\n",
      "    \"# Text Classification\\n\",\n",
      "    \"\\n\",\n",
      "    \"\\n\",\n",
      "    \"by   \\n\",\n",
      "    \"[__Michael Granitzer__ (michael.granitzer@uni-passau.de)]( http://www.mendeley.com/profiles/michael-granitzer/)  \\n\",\n",
      "    \"[Konstantin Ziegler (konstantin.ziegler@uni-passau.de)](http://zieglerk.net)  \\n\",\n",
      "    \"JÃ¶r\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "# simple file reading\n",
    "with open('./Bayes-Learning.ipynb') as f:\n",
    "    doc = f.read()\n",
    "    print(doc[80:366])\n",
    "    \n",
    "# codecs can help if you run into encoding problems\n",
    "with codecs.open('./Bayes-Learning.ipynb', encoding='latin1') as f:\n",
    "    doc = f.read()\n",
    "    print(doc[80:366])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: mathew <mathew@mantis.co.uk>\\n',\n",
      " 'Subject: Alt.Atheism FAQ: Atheist Resources\\n',\n",
      " 'Summary: Books, addresses, music -- anything related to atheism\\n',\n",
      " 'Keywords: FAQ, atheism, books, music, fiction, addresses, contacts\\n',\n",
      " 'Expires: Thu, 29 Apr 1993 11:57:19 GMT\\n',\n",
      " 'Distribution: world\\n',\n",
      " 'Organization: Mantis Consultants, Cambridge. UK.\\n',\n",
      " 'Supersedes: <19930301143317@mantis.co.uk>\\n',\n",
      " 'Lines: 290\\n',\n",
      " '\\n',\n",
      " 'Archive-name: atheism/resources\\n',\n",
      " 'Alt-atheism-archive-name: resources\\n',\n",
      " 'Last-modified: 11 December 1992\\n',\n",
      " 'Version: 1.0\\n',\n",
      " '\\n',\n",
      " '                              Atheist Resources\\n',\n",
      " '\\n',\n",
      " '                      Addresses of Atheist Organizations\\n',\n",
      " '\\n',\n",
      " '                                     USA\\n',\n",
      " '\\n',\n",
      " 'FREEDOM FROM RELIGION FOUNDATION\\n',\n",
      " '\\n',\n",
      " 'Darwin fish bumper stickers and assorted other atheist paraphernalia are\\n',\n",
      " 'available from the Freedom From Religion Foundation in the US.\\n',\n",
      " '\\n',\n",
      " 'Write to:  FFRF, P.O. Box 750, Madison, WI 53701.\\n',\n",
      " 'Telephone: (608) 256-8900\\n',\n",
      " '\\n',\n",
      " 'EVOLUTION DESIGNS\\n',\n",
      " '\\n',\n",
      " 'Evolution Designs sell the \"Darwin fish\".  It\\'s a fish symbol, like the '\n",
      " 'ones\\n',\n",
      " 'Christians stick on their cars, but with feet and the word \"Darwin\" '\n",
      " 'written\\n',\n",
      " 'inside.  The deluxe moulded 3D plastic fish is $4.95 postpaid in the US.\\n',\n",
      " '\\n',\n",
      " 'Write to:  Evolution Designs, 7119 Laurel Canyon #4, North Hollywood,\\n',\n",
      " '           CA 91605.\\n',\n",
      " '\\n',\n",
      " 'People in the San Francisco Bay area can get Darwin Fish from Lynn Gold --\\n',\n",
      " 'try mailing <figmo@netcom.com>.  For net people who go to Lynn directly, '\n",
      " 'the\\n']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "with codecs.open('./../1-Datasets_Visualization_and_preprocessing/newsgroups/20news-bydate-train/alt.atheism/49960', encoding='latin1') as file:\n",
    "    pprint.pprint(file.readlines()[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(doc_file):\n",
    "    with codecs.open(doc_file, encoding='latin1') as doc:\n",
    "        doc = doc.read().lower()\n",
    "        _header, _blankline, body = doc.partition('\\n\\n')\n",
    "        return re.findall(r'\\b\\w\\w+\\b',body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['archive',\n",
      " 'name',\n",
      " 'atheism',\n",
      " 'resources',\n",
      " 'alt',\n",
      " 'atheism',\n",
      " 'archive',\n",
      " 'name',\n",
      " 'resources',\n",
      " 'last',\n",
      " 'modified',\n",
      " '11',\n",
      " 'december',\n",
      " '1992',\n",
      " 'version',\n",
      " 'atheist',\n",
      " 'resources',\n",
      " 'addresses',\n",
      " 'of',\n",
      " 'atheist',\n",
      " 'organizations',\n",
      " 'usa',\n",
      " 'freedom',\n",
      " 'from',\n",
      " 'religion',\n",
      " 'foundation',\n",
      " 'darwin',\n",
      " 'fish',\n",
      " 'bumper',\n",
      " 'stickers']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(tokenize('./../1-Datasets_Visualization_and_preprocessing/newsgroups/20news-bydate-train/alt.atheism/49960')[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import codecs\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, min_count=1):\n",
    "        self.min_count = min_count\n",
    "        self.vocabulary = {}\n",
    "        self.num_docs = 0\n",
    "        self.classes = {}\n",
    "        self.priors = {}\n",
    "        self.conditionals = {}\n",
    "\n",
    "    def train(self, path):\n",
    "        self.num_docs = 0\n",
    "        for d in os.listdir(path):\n",
    "            self.classes[d] = {'doc_counts':0, 'terms':{}}\n",
    "            print(d)\n",
    "            for f in os.listdir(path + d):\n",
    "                terms = tokenize(path + d + '/' + f)\n",
    "                self.num_docs += 1\n",
    "                self.classes[d]['doc_counts'] += 1\n",
    "                \n",
    "                # build vocabulary and count terms\n",
    "                for term in terms:\n",
    "                    if not term in self.vocabulary:\n",
    "                        self.vocabulary[term] = 1\n",
    "                        self.classes[d]['terms'][term] = 1\n",
    "                    else:\n",
    "                        self.vocabulary[term] += 1\n",
    "                        if not term in self.classes[d]['terms']:\n",
    "                            self.classes[d]['terms'][term] = 1\n",
    "                        else:\n",
    "                            self.classes[d]['terms'][term] += 1\n",
    "                            \n",
    "        # remove terms with frequency < min_count\n",
    "        self.vocabulary = {k:v for k,v in self.vocabulary.items() if v > self.min_count}\n",
    "\n",
    "        for c in self.classes:\n",
    "            # calculate priors\n",
    "            self.priors[c] = math.log(self.classes[c]['doc_counts']) - math.log(self.num_docs)\n",
    "            \n",
    "            # calculate conditionals\n",
    "            self.conditionals[c] = {}\n",
    "            c_len = sum([self.classes[c]['terms'][x] for x in self.classes[c]['terms']])\n",
    "            for term in self.vocabulary:\n",
    "                t_ct = 1\n",
    "                if term in self.classes[c]['terms']:\n",
    "                    t_ct += self.classes[c]['terms'][term]\n",
    "                self.conditionals[c][term] = math.log(t_ct) - math.log(c_len + len(self.vocabulary))\n",
    "\n",
    "    def classify(self, doc):\n",
    "        scores = {}\n",
    "        for c in self.classes:\n",
    "            scores[c] = self.priors[c]\n",
    "            for term in doc:\n",
    "                if term in self.vocabulary:\n",
    "                    scores[c] += self.conditionals[c][term]\n",
    "\n",
    "        return scores, max(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n",
      "accuracy 0.757302177377\n"
     ]
    }
   ],
   "source": [
    "clf = NaiveBayesClassifier()\n",
    "\n",
    "clf.train('./../1-Datasets_Visualization_and_preprocessing/newsgroups/20news-bydate-train/')\n",
    "\n",
    "\n",
    "test_path = './../1-Datasets_Visualization_and_preprocessing/newsgroups/20news-bydate-test/'\n",
    "\n",
    "out_y = []\n",
    "true_y = []\n",
    "for cl in clf.classes:\n",
    "    for f in os.listdir(test_path + cl):\n",
    "        _, result_class = clf.classify(tokenize(test_path+cl+'/'+f))\n",
    "        out_y.append(result_class)\n",
    "        true_y.append(cl)\n",
    "\n",
    "print('accuracy',metrics.accuracy_score(true_y,out_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 130107)\n",
      "accuracy 0.772835900159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "twenty_train = load_files('./../1-Datasets_Visualization_and_preprocessing/newsgroups/20news-bydate-train/', encoding='latin1')\n",
    "twenty_test = load_files('./../1-Datasets_Visualization_and_preprocessing/newsgroups/20news-bydate-test/', encoding='latin1')\n",
    "\n",
    "vectorizer = feature_extraction.text.CountVectorizer()\n",
    "train_X = vectorizer.fit_transform(twenty_train.data)\n",
    "print(train_X.shape)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X,twenty_train.target)\n",
    "\n",
    "pred = clf.predict(vectorizer.transform(twenty_test.data))\n",
    "\n",
    "print('accuracy',metrics.accuracy_score(twenty_test.target,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.817843866171\n",
      "clf__alpha : 0.1\n",
      "tfidf__use_idf : True\n",
      "vect__ngram_range : (1, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', MultinomialNB()),\n",
    "               ])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1.0, 0.1),\n",
    "             }\n",
    "\n",
    "gs_clf = GridSearchCV(clf, parameters, n_jobs=-1)\n",
    "gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "pred = gs_clf.predict(twenty_test.data)\n",
    "print('accuracy',metrics.accuracy_score(twenty_test.target,pred))\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(param_name,\":\", gs_clf.best_params_[param_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/ds/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.834838024429\n",
      "clf__alpha : 0.001\n",
      "tfidf__use_idf : True\n",
      "vect__ngram_range : (1, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "#SGDClassifier with hinge loss gives a linear SVM\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42)),\n",
    "               ])\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "             }\n",
    "\n",
    "gs_clf = GridSearchCV(clf, parameters, n_jobs=-1)\n",
    "gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "pred = gs_clf.predict(twenty_test.data)\n",
    "print('accuracy',metrics.accuracy_score(twenty_test.target,pred))\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(param_name,\":\", gs_clf.best_params_[param_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "further details can be found at http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "449px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
